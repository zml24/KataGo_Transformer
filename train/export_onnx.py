#!/usr/bin/python3
import sys
import os
import argparse
import logging
import json
import datetime
import numpy as np
import torch
import torch.onnx
from typing import Dict, List, Optional, Tuple

import modelconfigs
from model_pytorch import Model
from load_model import load_model

#torch.backends.mha.set_fastpath_enabled(False) # transformer model will have bugs, so set false
# 定义手动实现的 forward 函数
def manual_rms_norm_forward(self, x):
   # 1. 强制 FP32 以保证数值稳定 (Crucial for LLMs)
    x_f32 = x.float()
    
    # 2. 使用乘法代替 pow(2)，效率略高且算子最简单
    # mean(-1) 计算均方值
    mean_square = (x_f32 * x_f32).mean(-1, keepdim=True)
    
    # 3. 计算 rsqrt (1 / sqrt(x))
    inv_rms = torch.rsqrt(mean_square + self.eps)
    
    # 4. 乘回原类型输入，并应用 gamma 参数
    # 注意：最终乘法建议在原精度下做，或者全在 FP32 做完再转回
    return self.weight * (x_f32 * inv_rms).type_as(x)

# 将 torch.nn.RMSNorm 类的 forward 方法替换为我们的手动实现
# 这样当导出器遍历模型时，看到的是基础数学运算，而不是 aten::rms_norm
if hasattr(torch.nn, "RMSNorm"):
    original_rms_norm_forward = torch.nn.RMSNorm.forward
    torch.nn.RMSNorm.forward = manual_rms_norm_forward

debug_mode=False
# Command and args -------------------------------------------------------------------

description = """
Export PyTorch neural net weights to ONNX format for inference.
"""

# Command line arguments will be parsed in the main block


class ONNXExportWrapper(torch.nn.Module):
    """
    Wrapper class to handle the model's forward pass for ONNX export.
    This handles the complex output structure and makes it ONNX-compatible.
    """
    
    def __init__(self, model: Model, disable_mask: bool):
        super(ONNXExportWrapper, self).__init__()
        self.model = model
        self.has_intermediate_head = model.get_has_intermediate_head()
        self.has_metadata_encoder = model.get_has_metadata_encoder()    
        self.disable_mask = disable_mask
    
    def forward(self, input_spatial: torch.Tensor, input_global: torch.Tensor, 
                input_meta: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, ...]:
        """
        Forward pass that returns a flattened tuple of outputs for ONNX compatibility.
        """
        # Call the original model
        if self.has_metadata_encoder and input_meta is not None:
            outputs = self.model(input_spatial, input_global, input_meta, disable_mask=disable_mask)
        else:
            outputs = self.model(input_spatial, input_global,disable_mask=disable_mask)
        outputs=outputs[0]
        
        pruned_outputs = tuple([outputs[i] for i in [0, 1, 2, 3, 4]])
        return pruned_outputs


def export_to_onnx(model: Model, save_name: str ,export_path: str, pos_len: int = 19, 
                   batch_size: int = 1, opset_version: int = 20, disable_mask: bool = False,
                   verbose: bool = False, extra_meta_data: Dict[str, str] = None,
                   auto_fp16: bool = False) -> None:
    """
    Export PyTorch model to ONNX format.
    
    Args:
        model: The PyTorch model to export
        export_path: Path to save the ONNX model
        pos_len: Board position length
        batch_size: Batch size for the model
        opset_version: ONNX opset version
        verbose: Whether to enable verbose logging
        auto_fp16: Whether to automatically convert to mixed precision (FP16)
    """
    
    # Set model to evaluation mode
    model.eval()
    
    # Create wrapper for ONNX export
    wrapper = ONNXExportWrapper(model,disable_mask)
    wrapper.eval()
    
    # Create dummy inputs
    num_spatial_inputs = modelconfigs.get_num_bin_input_features(model.config)
    input_spatial = torch.randn(batch_size, num_spatial_inputs, pos_len, pos_len, dtype=torch.float32)
    input_spatial[:,0,:,:]=1.0
    num_global_inputs = modelconfigs.get_num_global_input_features(model.config)
    input_global = torch.randn(batch_size, num_global_inputs, dtype=torch.float32)
    
    # Prepare inputs and input names
    inputs = [input_spatial, input_global]
    input_names = ['input_spatial', 'input_global']
    
    # Add metadata input if the model supports it
    if wrapper.has_metadata_encoder:
        # Assuming metadata has some standard size - this might need adjustment
        # based on the actual metadata encoder configuration
        input_meta = torch.randn(batch_size, 32, dtype=torch.float32)  # Placeholder size
        inputs.append(input_meta)
        input_names.append('input_meta')
    
    #output_names = [
    #    'policy', 'value', 'miscvalue', 'moremiscvalue', 
    #    'ownership', 'scoring', 'futurepos', 'seki', 'scorebelief_logprobs'
    #]
    output_names = [
        'out_policy', 'out_value', 'out_miscvalue', 'out_moremiscvalue', 
        'out_ownership'
    ]
    
    # Dynamic axes for variable batch size
    dynamic_axes = {}
    for name in input_names:
        dynamic_axes[name] = {0: 'batch_size'}
    for name in output_names:
        dynamic_axes[name] = {0: 'batch_size'}
    
    # Export to ONNX
    logging.info(f"Exporting model to ONNX format: {export_path}")
    logging.info(f"Input shapes: spatial={list(input_spatial.shape)}, global={list(input_global.shape)}")
    
    dynamo=False # now it does not support True
    report=False
    if dynamo:
        dynamic_axes=None
        report=True


    with torch.no_grad():
        torch.onnx.export(
            wrapper,
            tuple(inputs),
            export_path,
            export_params=True,
            opset_version=opset_version,
            do_constant_folding=True,
            input_names=input_names,
            output_names=output_names,
            dynamic_axes=dynamic_axes,
            verbose=verbose,
            dynamo=dynamo,
            report=report
        )
    
    logging.info("ONNX export completed successfully!")

    # Auto convert to FP16 if requested
    if auto_fp16:
        assert False,"Currently has trouble converting RMSNorm to fp16. Do not use -auto-fp16"
        
        import onnx
        from onnxconverter_common import auto_convert_mixed_precision,float16

        logging.info("Converting model to auto mixed precision (FP16)...")
        
        onnx_model = onnx.load(export_path)
        # Create feed_dict with dummy inputs converted to numpy
        feed_dict = {name: tensor.detach().cpu().numpy() for name, tensor in zip(input_names, inputs)}
        onnx_model_fp16 = auto_convert_mixed_precision(onnx_model, feed_dict, rtol=0.01, keep_io_types=True)
        #onnx_model_fp16 = float16.convert_float_to_float16(onnx_model)
        onnx.save(onnx_model_fp16, export_path)
        logging.info("Converted to FP16 successfully.")
            

    # Add metadata to the ONNX model
    try:
        import onnx
        from onnx import helper
        
        onnx_model = onnx.load(export_path)
        
        # Add metadata_props
        meta = {
            "name": save_name,
            "modelVersion": str(model.config["version"]),
            # Add other useful info if available
            "exported_at": datetime.datetime.now().isoformat(),
            "auto_fp16_already": "true" if auto_fp16 else "false",
            "opset_version": str(opset_version),
            "exported_with_dynamo": "true" if dynamo else "false",
            "num_spatial_inputs": str(num_spatial_inputs),
            "num_global_inputs": str(num_global_inputs),
            "pos_len": str(pos_len),
            "pos_len_x": str(pos_len),
            "pos_len_y": str(pos_len),
            "has_mask": "true" if not disable_mask else "false",
            "model_config": str(model.config)
        }
        if extra_meta_data is not None:
            meta.update(extra_meta_data)
        
        # Clear existing metadata if any to avoid duplicates
        if hasattr(onnx_model, "metadata_props"):
            del onnx_model.metadata_props[:]
            
        for key, value in meta.items():
            meta_entry = onnx_model.metadata_props.add()
            meta_entry.key = key
            meta_entry.value = value
            
        # Save the model with metadata
        onnx.save(onnx_model, export_path)
        logging.info(f"Added metadata to ONNX model: {meta}")
        
    except ImportError:
        logging.warning("onnx package not installed, skipping metadata addition")
    except Exception as e:
        logging.error(f"Failed to add metadata: {e}")


def verify_onnx_model(onnx_path: str, original_model: Model, pos_len: int = 19, 
                      batch_size: int = 1, ignore_intermediate_head: bool = True) -> bool:
    torch.nn.RMSNorm.forward = original_rms_norm_forward
    """
    Verify that the ONNX model produces similar outputs to the original PyTorch model.
    
    Args:
        onnx_path: Path to the ONNX model
        original_model: Original PyTorch model
        pos_len: Board position length
        batch_size: Batch size for testing
        ignore_intermediate_head: Whether to ignore intermediate head outputs
        
    Returns:
        True if verification passes, False otherwise
    """
    try:
        import onnxruntime as ort
    except ImportError:
        logging.warning("onnxruntime not available, skipping verification")
        return True
    
    # Load ONNX model
    ort_session = ort.InferenceSession(onnx_path)
    
    # Create test inputs
    num_spatial_inputs = modelconfigs.get_num_bin_input_features(model.config)
    input_spatial = torch.randn(batch_size, num_spatial_inputs, pos_len, pos_len, dtype=torch.float32)
    input_spatial[:,0,:,:]=1.0
    num_global_inputs = modelconfigs.get_num_global_input_features(model.config)
    input_global = torch.randn(batch_size, num_global_inputs, dtype=torch.float32)
    
    # Get PyTorch outputs
    original_model.eval()
    with torch.no_grad():
        if original_model.get_has_metadata_encoder():
            # For models with metadata encoder, we need to handle this case
            pytorch_outputs = original_model(input_spatial, input_global)
        else:
            pytorch_outputs = original_model(input_spatial, input_global)
    pytorch_outputs = pytorch_outputs[0]
    pytorch_outputs = [pytorch_outputs[i] for i in [0, 1, 2, 3, 4]]

    # Prepare ONNX inputs
    onnx_inputs = {
        'input_spatial': input_spatial.numpy(),
        'input_global': input_global.numpy()
    }
    
    # Get ONNX outputs
    onnx_outputs = ort_session.run(None, onnx_inputs)
    
    
    # Check if number of outputs match
    if len(onnx_outputs) != len(pytorch_outputs):
        logging.error(f"Output count mismatch: ONNX={len(onnx_outputs)}, PyTorch={len(pytorch_outputs)}")
        return False
    
    # Check output shapes and values
    for i, (onnx_out, pytorch_out) in enumerate(zip(onnx_outputs, pytorch_outputs)):
        pytorch_np = pytorch_out.detach().numpy()
        #print(onnx_out[0], pytorch_out[0])
        
        if onnx_out.shape != pytorch_np.shape:
            logging.error(f"Output {i} shape mismatch: ONNX={onnx_out.shape}, PyTorch={pytorch_np.shape}")
            return False
        
        # Check if values are close (allowing for small numerical differences)
        if not np.allclose(onnx_out, pytorch_np, rtol=1e-5, atol=1e-6):
            max_diff = np.max(np.abs(onnx_out - pytorch_np))
            logging.warning(f"Output {i} values differ, max difference: {max_diff}")
            # Don't fail on small differences, just warn
    
    logging.info("ONNX model verification passed!")
    return True


if __name__ == "__main__":

    if not debug_mode:
        parser = argparse.ArgumentParser(description=description)
        parser.add_argument('-checkpoint', help='Checkpoint file to export', required=True)
        parser.add_argument('-export-dir', help='Directory to export ONNX model to', required=True)
        parser.add_argument('-model-name', help='Name for the exported model', required=True)
        parser.add_argument('-use-swa', help='Use SWA model if available', action='store_true', required=False)
        parser.add_argument('-pos-len', help='Spatial edge length (e.g. 19 for 19x19 Go)', type=int, default=19, required=False)
        parser.add_argument('-batch-size', help='Batch size for ONNX export', type=int, default=4, required=False)
        parser.add_argument('-opset-version', help='ONNX opset version', type=int, default=20, required=False)
        parser.add_argument('-simplify', help='Simplify ONNX model using onnx-simplifier', action='store_true', required=False)
        parser.add_argument('-disable-mask', help='Disable masks in CNN and attention', action='store_true', required=False)
        parser.add_argument('-auto-fp16', help='Convert to half precision (FP16) automatically', action='store_true', required=False)
        parser.add_argument('-verbose', help='Verbose output', action='store_true', required=False)
        parser.add_argument('-author', help='Author name for metadata', required=False,default="unknown")
        parser.add_argument('-comment', help='Comment for metadata', required=False,default="")
        
        args = parser.parse_args()



        checkpoint_file = args.checkpoint
        export_dir = args.export_dir
        model_name = args.model_name
        use_swa = args.use_swa
        pos_len = args.pos_len
        batch_size = args.batch_size
        opset_version = args.opset_version
        simplify = args.simplify
        disable_mask = args.disable_mask
        auto_fp16 = args.auto_fp16
        verbose = args.verbose
        author = args.author
        comment = args.comment
        extra_meta_data = {}
        if author is not None:
            extra_meta_data["author"] = author
        if comment is not None:
            extra_meta_data["comment"] = comment
    else:
        checkpoint_file = "../data/train/go_b24c128tf1b_muon1_fd1/checkpoint.ckpt"
        export_dir = "../onnx_exports"
        model_name = "go_b24c128tf1b_muon1_fd1"
        use_swa = True
        pos_len = 19
        batch_size = 128
        opset_version = 20
        simplify = False
        auto_fp16 = False
        verbose = False
    
    # Create export directory
    os.makedirs(export_dir, exist_ok=True)
    
    # Set up logging
    logging.root.handlers = []
    logging.basicConfig(
        level=logging.INFO,
        format="%(message)s",
        handlers=[
            logging.StreamHandler(stream=sys.stdout),
            logging.FileHandler(os.path.join(export_dir, "export_log.txt")),
        ],
    )
    
    logging.info(f"ONNX Export Script - {datetime.datetime.now()}")
    logging.info(f"Arguments: {sys.argv}")
    
    # Load model
    logging.info(f"Loading model from checkpoint: {checkpoint_file}")
    model, swa_model, other_state_dict = load_model(
        checkpoint_file, use_swa, device="cpu", pos_len=pos_len, verbose=True
    )
    
    # Use SWA model if requested and available
    export_model = swa_model if (use_swa and swa_model is not None) else model
    model_type = "SWA" if (use_swa and swa_model is not None) else "regular"
    
    logging.info(f"Exporting {model_type} model")
    logging.info(f"Model config: {export_model.config}")
    
    # Export to ONNX
    save_name = f"{model_name}"
    # Add training state info if available
    if "train_state" in other_state_dict:
        train_state = other_state_dict["train_state"]
        if "global_step_samples" in train_state:
            save_name += f"-s{train_state['global_step_samples']}"
        if "total_num_data_rows" in train_state:
            save_name += f"-d{train_state['total_num_data_rows']}"
    onnx_filename = f"{save_name}.onnx"
    onnx_path = os.path.join(export_dir, onnx_filename)
    
    export_to_onnx(
        export_model, 
        save_name,
        onnx_path, 
        pos_len=pos_len, 
        batch_size=batch_size, 
        opset_version=opset_version,
        disable_mask=disable_mask,
        verbose=verbose,
        extra_meta_data=extra_meta_data,
        auto_fp16=auto_fp16
    )
    
    # Verify the exported model
    logging.info("Verifying exported ONNX model...")
    verification_passed = verify_onnx_model(onnx_path, export_model, pos_len, batch_size)
    
    if not verification_passed:
        logging.error("ONNX model verification failed!")
        exit(1)
    
    # Simplify model if requested
    if simplify:
        import onnxsim
        logging.info("Simplifying ONNX model...")
        simplified_path = os.path.join(export_dir, f"{model_name}_simplified.onnx")
        onnxsim.simplify(onnx_path, simplified_path)
        logging.info(f"Simplified model saved to: {simplified_path}")
    
    # Save metadata
    metadata = {
        "model_name": model_name,
        "export_time": datetime.datetime.now().isoformat(),
        "checkpoint_file": checkpoint_file,
        "model_type": model_type,
        "pos_len": pos_len,
        "batch_size": batch_size,
        "opset_version": opset_version,
        "has_intermediate_head": export_model.get_has_intermediate_head(),
        "has_metadata_encoder": export_model.get_has_metadata_encoder(),
        "model_config": export_model.config
    }
    
    # Add training state info if available
    if "train_state" in other_state_dict:
        train_state = other_state_dict["train_state"]
        if "global_step_samples" in train_state:
            metadata["global_step_samples"] = train_state["global_step_samples"]
        if "total_num_data_rows" in train_state:
            metadata["total_num_data_rows"] = train_state["total_num_data_rows"]
    
    metadata_path = os.path.join(export_dir, f"{model_name}_metadata.json")
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=2)
    
    logging.info(f"Export completed successfully!")
    logging.info(f"ONNX model: {onnx_path}")
    logging.info(f"Metadata: {metadata_path}")
    
    exit(0)

