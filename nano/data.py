"""Data loading and symmetry augmentation for KataGo nano training."""

import logging
import os
import threading
import queue

import numpy as np
from concurrent.futures import ThreadPoolExecutor

import torch
import torch.nn.functional

import configs


def prefetch_generator(gen, prefetch_batches):
    """Prefetch batches in a background thread to overlap data prep with GPU compute."""
    if prefetch_batches <= 0:
        yield from gen
        return

    q = queue.Queue(maxsize=prefetch_batches)
    _sentinel = object()

    def _producer():
        try:
            for item in gen:
                q.put(item)
        except Exception as e:
            q.put(e)
        finally:
            q.put(_sentinel)

    t = threading.Thread(target=_producer, daemon=True)
    t.start()

    while True:
        item = q.get()
        if item is _sentinel:
            break
        if isinstance(item, Exception):
            raise item
        yield item

    t.join()


def read_npz_training_data(
    npz_files,
    batch_size: int,
    world_size: int,
    rank: int,
    pos_len: int,
    device,
    symmetry_type: str,
    include_meta: bool,
    enable_history_matrices: bool,
    model_config: configs.ModelConfig,
    use_pin_memory: bool = False,
):
    rand = np.random.default_rng(seed=list(os.urandom(12)))
    num_bin_features = configs.get_num_bin_input_features(model_config)
    num_global_features = configs.get_num_global_input_features(model_config)
    if enable_history_matrices:
        (h_base, h_builder) = build_history_matrices(model_config, device)

    include_qvalues = model_config["version"] >= 16 and model_config["version"] < 100

    def load_npz_file(npz_file):
        with np.load(npz_file) as npz:
            is_preprocessed = "binaryInputNCHW" in npz

            if is_preprocessed:
                # Preprocessed format: already unpacked uint8 NCHW
                binaryInputNCHW = npz["binaryInputNCHW"]
                stored_pos_len = int(npz["pos_len"]) if "pos_len" in npz else None
                if stored_pos_len is not None:
                    assert stored_pos_len == pos_len, \
                        f"pos_len mismatch: file has {stored_pos_len}, expected {pos_len}"
                assert binaryInputNCHW.shape[2] == pos_len and binaryInputNCHW.shape[3] == pos_len, \
                    f"Spatial dims {binaryInputNCHW.shape[2:]}, expected ({pos_len}, {pos_len})"
            else:
                # Original format: packed bits, need unpackbits
                binaryInputNCHWPacked = npz["binaryInputNCHWPacked"]
                binaryInputNCHW = np.unpackbits(binaryInputNCHWPacked, axis=2)
                assert len(binaryInputNCHW.shape) == 3
                assert binaryInputNCHW.shape[2] == ((pos_len * pos_len + 7) // 8) * 8
                binaryInputNCHW = binaryInputNCHW[:, :, :pos_len * pos_len]
                binaryInputNCHW = np.reshape(binaryInputNCHW, (
                    binaryInputNCHW.shape[0], binaryInputNCHW.shape[1], pos_len, pos_len
                ))  # uint8, no float32 conversion here

            globalInputNC = npz["globalInputNC"]
            policyTargetsNCMove = npz["policyTargetsNCMove"]
            globalTargetsNC = npz["globalTargetsNC"]
            scoreDistrN = npz["scoreDistrN"]
            valueTargetsNCHW = npz["valueTargetsNCHW"]
            if include_meta:
                metadataInputNC = npz["metadataInputNC"]
            else:
                metadataInputNC = None
            if include_qvalues:
                qValueTargetsNCMove = npz["qValueTargetsNCMove"]
            else:
                qValueTargetsNCMove = None

        assert binaryInputNCHW.shape[1] == num_bin_features
        assert globalInputNC.shape[1] == num_global_features
        return (npz_file, binaryInputNCHW, globalInputNC, policyTargetsNCMove, globalTargetsNC, scoreDistrN, valueTargetsNCHW, metadataInputNC, qValueTargetsNCMove)

    if not npz_files:
        return

    with ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(load_npz_file, npz_files[0])

        for next_file in (npz_files[1:] + [None]):
            (npz_file, binaryInputNCHW, globalInputNC, policyTargetsNCMove, globalTargetsNC, scoreDistrN, valueTargetsNCHW, metadataInputNC, qValueTargetsNCMove) = future.result()

            num_samples = binaryInputNCHW.shape[0]
            num_whole_steps = num_samples // (batch_size * world_size)

            if next_file is not None:
                future = executor.submit(load_npz_file, next_file)

            for n in range(num_whole_steps):
                start = (n * world_size + rank) * batch_size
                end = start + batch_size

                if use_pin_memory:
                    batch_binaryInputNCHW = torch.from_numpy(binaryInputNCHW[start:end]).pin_memory().to(device, non_blocking=True).float()
                    batch_globalInputNC = torch.from_numpy(globalInputNC[start:end]).pin_memory().to(device, non_blocking=True).float()
                    batch_policyTargetsNCMove = torch.from_numpy(policyTargetsNCMove[start:end]).pin_memory().to(device, non_blocking=True).float()
                    batch_globalTargetsNC = torch.from_numpy(globalTargetsNC[start:end]).pin_memory().to(device, non_blocking=True).float()
                    batch_scoreDistrN = torch.from_numpy(scoreDistrN[start:end]).pin_memory().to(device, non_blocking=True).float()
                    batch_valueTargetsNCHW = torch.from_numpy(valueTargetsNCHW[start:end]).pin_memory().to(device, non_blocking=True).float()
                    if include_meta:
                        batch_metadataInputNC = torch.from_numpy(metadataInputNC[start:end]).pin_memory().to(device, non_blocking=True).float()
                    if include_qvalues:
                        batch_qValueTargetsNCMove = torch.from_numpy(qValueTargetsNCMove[start:end]).pin_memory().to(device, non_blocking=True).float()
                else:
                    batch_binaryInputNCHW = torch.from_numpy(binaryInputNCHW[start:end]).to(device).float()
                    batch_globalInputNC = torch.from_numpy(globalInputNC[start:end]).to(device).float()
                    batch_policyTargetsNCMove = torch.from_numpy(policyTargetsNCMove[start:end]).to(device).float()
                    batch_globalTargetsNC = torch.from_numpy(globalTargetsNC[start:end]).to(device).float()
                    batch_scoreDistrN = torch.from_numpy(scoreDistrN[start:end]).to(device).float()
                    batch_valueTargetsNCHW = torch.from_numpy(valueTargetsNCHW[start:end]).to(device).float()
                    if include_meta:
                        batch_metadataInputNC = torch.from_numpy(metadataInputNC[start:end]).to(device).float()
                    if include_qvalues:
                        batch_qValueTargetsNCMove = torch.from_numpy(qValueTargetsNCMove[start:end]).to(device).float()

                if enable_history_matrices:
                    (batch_binaryInputNCHW, batch_globalInputNC) = apply_history_matrices(
                        model_config, batch_binaryInputNCHW, batch_globalInputNC, batch_globalTargetsNC, h_base, h_builder
                    )

                if symmetry_type is not None and symmetry_type != "" and symmetry_type != "none":
                    allowed_symms = []
                    if symmetry_type == "xyt":
                        allowed_symms = [0, 1, 2, 3, 4, 5, 6, 7]
                    elif symmetry_type == "x":
                        allowed_symms = [0, 5]
                    elif symmetry_type == "xy":
                        allowed_symms = [0, 2, 5, 7]
                    elif symmetry_type == "x+y":
                        allowed_symms = [0, 2]
                    elif symmetry_type == "t":
                        allowed_symms = [0, 4]
                    else:
                        assert False, f"Unknown data symmetry type {symmetry_type}"

                    symm = allowed_symms[int(rand.integers(0, len(allowed_symms)))]

                    batch_binaryInputNCHW = apply_symmetry(batch_binaryInputNCHW, symm)
                    batch_policyTargetsNCMove = apply_symmetry_policy(batch_policyTargetsNCMove, symm, pos_len)
                    batch_valueTargetsNCHW = apply_symmetry(batch_valueTargetsNCHW, symm)
                    if include_qvalues:
                        batch_qValueTargetsNCMove = apply_symmetry_policy(batch_qValueTargetsNCMove, symm, pos_len)

                batch_binaryInputNCHW = batch_binaryInputNCHW.contiguous()
                batch_policyTargetsNCMove = batch_policyTargetsNCMove.contiguous()
                batch_valueTargetsNCHW = batch_valueTargetsNCHW.contiguous()
                if include_qvalues:
                    batch_qValueTargetsNCMove = batch_qValueTargetsNCMove.contiguous()

                batch = dict(
                    binaryInputNCHW=batch_binaryInputNCHW,
                    globalInputNC=batch_globalInputNC,
                    policyTargetsNCMove=batch_policyTargetsNCMove,
                    globalTargetsNC=batch_globalTargetsNC,
                    scoreDistrN=batch_scoreDistrN,
                    valueTargetsNCHW=batch_valueTargetsNCHW,
                )
                if include_meta:
                    batch["metadataInputNC"] = batch_metadataInputNC
                if include_qvalues:
                    batch["qValueTargetsNCMove"] = batch_qValueTargetsNCMove

                yield batch


def apply_symmetry_policy(tensor, symm, pos_len):
    """Same as apply_symmetry but also handles the pass index."""
    batch_size = tensor.shape[0]
    channels = tensor.shape[1]
    tensor_without_pass = tensor[:, :, :-1].view((batch_size, channels, pos_len, pos_len))
    tensor_transformed = apply_symmetry(tensor_without_pass, symm)
    return torch.cat((
        tensor_transformed.reshape(batch_size, channels, pos_len * pos_len),
        tensor[:, :, -1:]
    ), dim=2)


def apply_symmetry(tensor, symm):
    """
    Apply a symmetry operation to the given tensor.

    Args:
        tensor (torch.Tensor): Tensor to be rotated. (..., W, W)
        symm (int):
            0, 1, 2, 3: Rotation by symm * pi / 2 radians.
            4, 5, 6, 7: Mirror symmetry on top of rotation.
    """
    assert tensor.shape[-1] == tensor.shape[-2]

    if symm == 0:
        return tensor
    if symm == 1:
        return tensor.transpose(-2, -1).flip(-2)
    if symm == 2:
        return tensor.flip(-1).flip(-2)
    if symm == 3:
        return tensor.transpose(-2, -1).flip(-1)
    if symm == 4:
        return tensor.transpose(-2, -1)
    if symm == 5:
        return tensor.flip(-1)
    if symm == 6:
        return tensor.transpose(-2, -1).flip(-1).flip(-2)
    if symm == 7:
        return tensor.flip(-2)


def build_history_matrices(model_config: configs.ModelConfig, device):
    num_bin_features = configs.get_num_bin_input_features(model_config)
    assert num_bin_features == 22, "Currently this code is hardcoded for this many features"

    h_base = torch.diag(
        torch.tensor(
            [
                1.0,  # 0
                1.0,  # 1
                1.0,  # 2
                1.0,  # 3
                1.0,  # 4
                1.0,  # 5
                1.0,  # 6
                1.0,  # 7
                1.0,  # 8
                0.0,  # 9   Location of move 1 turn ago
                0.0,  # 10  Location of move 2 turns ago
                0.0,  # 11  Location of move 3 turns ago
                0.0,  # 12  Location of move 4 turns ago
                0.0,  # 13  Location of move 5 turns ago
                1.0,  # 14  Ladder-threatened stone
                0.0,  # 15  Ladder-threatened stone, 1 turn ago
                0.0,  # 16  Ladder-threatened stone, 2 turns ago
                1.0,  # 17
                1.0,  # 18
                1.0,  # 19
                1.0,  # 20
                1.0,  # 21
            ],
            device=device,
            requires_grad=False,
        )
    )
    h_base[14, 15] = 1.0
    h_base[14, 16] = 1.0

    h0 = torch.zeros(num_bin_features, num_bin_features, device=device, requires_grad=False)
    h0[9, 9] = 1.0
    h0[14, 15] = -1.0
    h0[14, 16] = -1.0
    h0[15, 15] = 1.0
    h0[15, 16] = 1.0

    h1 = torch.zeros(num_bin_features, num_bin_features, device=device, requires_grad=False)
    h1[10, 10] = 1.0
    h1[15, 16] = -1.0
    h1[16, 16] = 1.0

    h2 = torch.zeros(num_bin_features, num_bin_features, device=device, requires_grad=False)
    h2[11, 11] = 1.0

    h3 = torch.zeros(num_bin_features, num_bin_features, device=device, requires_grad=False)
    h3[12, 12] = 1.0

    h4 = torch.zeros(num_bin_features, num_bin_features, device=device, requires_grad=False)
    h4[13, 13] = 1.0

    h_base = h_base.reshape((1, num_bin_features, num_bin_features))
    h_builder = torch.stack((h0, h1, h2, h3, h4), dim=0)

    return (h_base, h_builder)


def apply_history_matrices(model_config, batch_binaryInputNCHW, batch_globalInputNC, batch_globalTargetsNC, h_base, h_builder):
    num_global_features = configs.get_num_global_input_features(model_config)
    # Generate random on CPU to avoid conflict with torch.compile CUDA graph capture
    ref = batch_globalTargetsNC[:, 36:41]
    should_stop_history = (torch.rand(ref.shape, dtype=ref.dtype, device="cpu") >= 0.98).to(ref.device)
    include_history = (torch.cumsum(should_stop_history, axis=1, dtype=torch.float32) <= 0.1).to(torch.float32)

    h_matrix = h_base + torch.einsum("bi,ijk->bjk", include_history, h_builder)

    batch_binaryInputNCHW = torch.einsum("bijk,bil->bljk", batch_binaryInputNCHW, h_matrix)

    batch_globalInputNC = batch_globalInputNC * torch.nn.functional.pad(
        include_history, ((0, num_global_features - include_history.shape[1])), value=1.0
    )
    return batch_binaryInputNCHW, batch_globalInputNC
